{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"4gO-7AXb6Gjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yeQGXuxU6KNu","executionInfo":{"status":"ok","timestamp":1679030819594,"user_tz":-540,"elapsed":16314,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"344d1ba3-2ac7-43d6-fa44-7428f6a5eee5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# UltraLytics YOLO v5 Custom Data Image Detection 직접하기"],"metadata":{"id":"mSpYwfyhmHtn"}},{"cell_type":"markdown","source":["## UltraLytics YOLO v5 설치\n","\n","![install](https://github.com/DrKAI/CV/raw/main/UltraLytics_manual/yolov5_install.png)\n","\n","[Install Page](https://github.com/ultralytics/yolov5)"],"metadata":{"id":"YB6S7Ad-km8I"}},{"cell_type":"code","source":["!git clone https://github.com/ultralytics/yolov5"],"metadata":{"id":"0BgUAeV0l4kL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679030821154,"user_tz":-540,"elapsed":1566,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"2e36c526-4fc0-4de7-debe-7fd4b24f19c2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15305, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (5/5), done.\u001b[K\n","remote: Total 15305 (delta 0), reused 3 (delta 0), pack-reused 15300\u001b[K\n","Receiving objects: 100% (15305/15305), 14.19 MiB | 32.22 MiB/s, done.\n","Resolving deltas: 100% (10496/10496), done.\n"]}]},{"cell_type":"code","source":["## yolov5 폴더 requirements.txt 수정 필요\n","## setuptools<=64.0.2\n","\n","temp_str = 'setuptools<=64.0.2\\n' \n","f = open('/content/yolov5/requirements.txt', 'r') \n","f_str = f.readlines() \n","f.close() \n","\n","f2 = open('/content/yolov5/requirements.txt', 'w') \n","\n","for idx, val in enumerate(f_str) : \n","    if 'setuptools' in val : \n","        idx_v = idx \n","        f_str.remove(val) \n","        f_str.insert(idx_v, temp_str) \n","\n","for val in f_str : \n","    f2.write(val) \n","    \n","f2.close()"],"metadata":{"id":"KFiEWQbg4Sf1","executionInfo":{"status":"ok","timestamp":1679030858275,"user_tz":-540,"elapsed":286,"user":{"displayName":"박지환","userId":"01747138341474302424"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!cd yolov5; pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVPHc9Cv4bUN","executionInfo":{"status":"ok","timestamp":1679030863784,"user_tz":-540,"elapsed":4747,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"7e994b42-e94a-4621-a1f2-245d04e20352"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/184.3 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 8)) (4.6.0.66)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 9)) (8.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (5.4.8)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 12)) (2.25.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 13)) (1.10.1)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 15)) (1.13.1+cu116)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 16)) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 17)) (4.65.0)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 21)) (2.11.2)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 26)) (1.4.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 27)) (0.12.2)\n","Requirement already satisfied: setuptools<=64.0.2 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 41)) (63.4.3)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.0.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (23.0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (5.12.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (4.39.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (1.26.15)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->-r requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.2.3)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.4.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.40.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.16.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.51.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.6)\n","Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.19.6)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 26)) (2022.7.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.3.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.15.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (6.0.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.1.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.2.2)\n","Installing collected packages: smmap, thop, gitdb, gitpython\n","Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0 thop-0.1.1.post2209072238\n"]}]},{"cell_type":"markdown","source":["## Image Detection\n","\n","1. **본인이 사전 작업한 CustomData yaml 다운로드**\n","\n","2. pretrained 된 weights 다운로드\n","    - weights가 없으면 자동 다운로드\n","\n","3. train.py 실행\n","\n","4. **테스트용 이미지 다운로드**\n","\n","4. detect.py 실행"],"metadata":{"id":"x49tFhjSmbOA"}},{"cell_type":"markdown","source":["### 1) **본인의 CustomData yaml 다운로드**\n","\n","- CustomData yaml 사전 작업 필요"],"metadata":{"id":"oG0KBwlqnR4A"}},{"cell_type":"code","source":[],"metadata":{"id":"i2iYAT6j5aaH","executionInfo":{"status":"ok","timestamp":1679028837619,"user_tz":-540,"elapsed":274,"user":{"displayName":"박지환","userId":"01747138341474302424"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 2) Pretrained weights 다운로드\n","\n","![Pretrained weights](https://github.com/DrKAI/CV/raw/main/UltraLytics_manual/yolov5_pretrained.png)\n","\n","[Pretrained weights Page](https://github.com/ultralytics/yolov5)"],"metadata":{"id":"1beFaHRDoAT5"}},{"cell_type":"code","source":["!mkdir /content/yolov5/pretrained"],"metadata":{"id":"4T8CcI0jmbCa","executionInfo":{"status":"ok","timestamp":1679030890555,"user_tz":-540,"elapsed":428,"user":{"displayName":"박지환","userId":"01747138341474302424"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!wget -O /content/yolov5/pretrained/yolov5m.pt https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQVjyLjo5c62","executionInfo":{"status":"ok","timestamp":1679030891151,"user_tz":-540,"elapsed":314,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"676e8cb4-0deb-4476-adf1-ae69b533b890"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-03-17 05:28:13--  https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/7acc87ed-9e1f-4d4a-8bdc-0912393948df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230317T052813Z&X-Amz-Expires=300&X-Amz-Signature=c8f5e04f5fcf7a295aa0d76addf4b609acc941d63cef96628df0a5df0c338f4c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5m.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-03-17 05:28:14--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/7acc87ed-9e1f-4d4a-8bdc-0912393948df?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230317%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230317T052813Z&X-Amz-Expires=300&X-Amz-Signature=c8f5e04f5fcf7a295aa0d76addf4b609acc941d63cef96628df0a5df0c338f4c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5m.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 42806829 (41M) [application/octet-stream]\n","Saving to: ‘/content/yolov5/pretrained/yolov5m.pt’\n","\n","\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/yolov5/pre 100%[===================>]  40.82M  --.-KB/s    in 0.1s    \n","\n","2023-03-17 05:28:14 (394 MB/s) - ‘/content/yolov5/pretrained/yolov5m.pt’ saved [42806829/42806829]\n","\n"]}]},{"cell_type":"markdown","source":["### 3) train.py 실행\n","\n","- 명령어 도움말 : python train.py -h"],"metadata":{"id":"54_FsNfxJK3R"}},{"cell_type":"code","source":["!cd yolov5; python train.py \\\n","    --data '/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian.yaml' \\\n","    --cfg '/content/yolov5/models/yolov5m.yaml' \\\n","    --weights '/content/yolov5/pretrained/yolov5m.pt' \\\n","    --epochs 1000 \\\n","    --patience 7 \\\n","    --project 'trained' \\\n","    --name 'train_chim_ju_kian'"],"metadata":{"id":"0pyVOfrXJW2L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679031104330,"user_tz":-540,"elapsed":204309,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"52cff880-4d96-46ff-bf3b-d457c38ec40f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/pretrained/yolov5m.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1000, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=trained, name=train_chim_ju_kian, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=7, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir trained', view at http://localhost:6006/\n","2023-03-17 05:28:30.276541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-17 05:28:31.414123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-17 05:28:31.414250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-17 05:28:31.414270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n","100% 755k/755k [00:00<00:00, 120MB/s]\n","Overriding model.yaml nc=80 with nc=3\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     32328  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","YOLOv5m summary: 291 layers, 20879400 parameters, 20879400 gradients, 48.2 GFLOPs\n","\n","Transferred 474/481 items from /content/yolov5/pretrained/yolov5m.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/labels/train... 31 images, 0 backgrounds, 0 corrupt: 100% 31/31 [00:22<00:00,  1.36it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/labels/train.cache\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/labels/train.cache... 31 images, 0 backgrounds, 0 corrupt: 100% 31/31 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.76 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to trained/train_chim_ju_kian/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mtrained/train_chim_ju_kian\u001b[0m\n","Starting training for 1000 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      0/999      6.09G     0.1191    0.04346    0.04011         77        640: 100% 2/2 [00:05<00:00,  2.77s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:03<00:00,  3.18s/it]\n","                   all         31         91   0.000659     0.0673   0.000398   0.000124\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      1/999      7.37G     0.1178    0.04351    0.04055         82        640: 100% 2/2 [00:01<00:00,  1.91it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:02<00:00,  2.13s/it]\n","                   all         31         91   0.000893     0.0885    0.00052   0.000144\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      2/999      7.37G      0.115    0.04904    0.04031         82        640: 100% 2/2 [00:01<00:00,  1.85it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.97s/it]\n","                   all         31         91   0.000975     0.0989    0.00057   0.000149\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      3/999      7.37G      0.109    0.04891    0.03974         83        640: 100% 2/2 [00:00<00:00,  2.20it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.42s/it]\n","                   all         31         91    0.00113     0.0996   0.000739   0.000186\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      4/999      7.37G     0.1024    0.05082    0.04011         80        640: 100% 2/2 [00:00<00:00,  2.15it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.48s/it]\n","                   all         31         91    0.00149      0.142    0.00104   0.000275\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      5/999      7.37G     0.0956    0.05689    0.03982         80        640: 100% 2/2 [00:01<00:00,  1.78it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:02<00:00,  2.09s/it]\n","                   all         31         91    0.00125      0.119   0.000975   0.000308\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      6/999      7.37G    0.09336    0.06799    0.03938         94        640: 100% 2/2 [00:00<00:00,  2.13it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.21s/it]\n","                   all         31         91    0.00215      0.215    0.00163   0.000457\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      7/999      7.37G    0.08424    0.06093    0.03875         73        640: 100% 2/2 [00:00<00:00,  2.14it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.26s/it]\n","                   all         31         91    0.00363      0.376     0.0036   0.000866\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      8/999      7.37G    0.08211    0.06418    0.03859         81        640: 100% 2/2 [00:00<00:00,  2.29it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.23s/it]\n","                   all         31         91    0.00464      0.483    0.00538    0.00131\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      9/999      7.37G    0.08121    0.06537    0.03798         79        640: 100% 2/2 [00:01<00:00,  1.86it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:02<00:00,  2.18s/it]\n","                   all         31         91    0.00577      0.587    0.00808    0.00215\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     10/999      7.37G    0.07885    0.06115    0.03859         78        640: 100% 2/2 [00:00<00:00,  2.26it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.22s/it]\n","                   all         31         91     0.0068      0.687     0.0112    0.00313\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     11/999      7.37G    0.07347    0.06291    0.03861         68        640: 100% 2/2 [00:00<00:00,  2.25it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.07s/it]\n","                   all         31         91    0.00825      0.825     0.0195    0.00523\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     12/999      7.37G    0.07507     0.0662    0.03769         84        640: 100% 2/2 [00:00<00:00,  2.19it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.15s/it]\n","                   all         31         91    0.00856      0.859     0.0311     0.0088\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     13/999      7.37G    0.07352    0.06527    0.03802         77        640: 100% 2/2 [00:01<00:00,  1.94it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.88s/it]\n","                   all         31         91    0.00886      0.892      0.052     0.0155\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     14/999      7.37G    0.07304    0.06283    0.03733         68        640: 100% 2/2 [00:00<00:00,  2.25it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.08s/it]\n","                   all         31         91    0.00898      0.913     0.0727     0.0227\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     15/999      7.37G    0.07265    0.06573    0.03737        113        640: 100% 2/2 [00:00<00:00,  2.18it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.17s/it]\n","                   all         31         91    0.00897      0.913     0.0945      0.024\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     16/999      7.37G     0.0698    0.06382    0.03707         85        640: 100% 2/2 [00:00<00:00,  2.19it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.11s/it]\n","                   all         31         91     0.0443       0.89      0.123     0.0385\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     17/999      7.37G    0.06723    0.06444    0.03676         86        640: 100% 2/2 [00:01<00:00,  1.97it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.69s/it]\n","                   all         31         91      0.122      0.344      0.139     0.0462\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     18/999      7.37G    0.06583    0.06703    0.03661        112        640: 100% 2/2 [00:01<00:00,  1.77it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.16it/s]\n","                   all         31         91      0.175      0.397      0.168     0.0573\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     19/999      7.37G    0.06342    0.05001    0.03628         69        640: 100% 2/2 [00:00<00:00,  2.34it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.15s/it]\n","                   all         31         91      0.169      0.452      0.179     0.0583\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     20/999      7.37G    0.06618    0.05416    0.03625         82        640: 100% 2/2 [00:00<00:00,  2.24it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.04s/it]\n","                   all         31         91      0.181      0.429      0.184     0.0668\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     21/999      7.37G    0.06196    0.04846    0.03659         75        640: 100% 2/2 [00:01<00:00,  1.85it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.63s/it]\n","                   all         31         91      0.183      0.439      0.181     0.0645\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     22/999      7.37G    0.07037    0.04908    0.03673         78        640: 100% 2/2 [00:00<00:00,  2.23it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.03it/s]\n","                   all         31         91     0.0768      0.298      0.121     0.0372\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     23/999      7.37G    0.06956    0.04527    0.03599         80        640: 100% 2/2 [00:00<00:00,  2.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.06it/s]\n","                   all         31         91      0.107      0.387       0.14     0.0394\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     24/999      7.37G    0.06794    0.05076    0.03618         90        640: 100% 2/2 [00:00<00:00,  2.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.02it/s]\n","                   all         31         91     0.0877      0.367     0.0852     0.0197\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     25/999      7.37G     0.0692    0.04429    0.03613         88        640: 100% 2/2 [00:00<00:00,  2.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.32s/it]\n","                   all         31         91      0.165      0.565      0.218     0.0785\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     26/999      7.37G      0.066    0.04709    0.03574         94        640: 100% 2/2 [00:01<00:00,  1.90it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.02s/it]\n","                   all         31         91     0.0281      0.315      0.036     0.0132\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     27/999      7.37G    0.07684    0.04084    0.03622         81        640: 100% 2/2 [00:00<00:00,  2.39it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.06it/s]\n","                   all         31         91     0.0281      0.315      0.036     0.0132\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     28/999      7.37G    0.07326    0.04542    0.03586         92        640: 100% 2/2 [00:00<00:00,  2.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.21it/s]\n","                   all         31         91      0.109       0.58      0.137     0.0484\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     29/999      7.37G    0.06813    0.03991    0.03591         58        640: 100% 2/2 [00:00<00:00,  2.25it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.15it/s]\n","                   all         31         91      0.179      0.392      0.209     0.0638\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     30/999      7.37G    0.07011    0.04969    0.03622         87        640: 100% 2/2 [00:00<00:00,  2.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.40s/it]\n","                   all         31         91      0.179      0.392      0.209     0.0638\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     31/999      7.37G    0.06505    0.04379     0.0358         91        640: 100% 2/2 [00:00<00:00,  2.05it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.20it/s]\n","                   all         31         91      0.188      0.632      0.256     0.0891\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     32/999      7.37G    0.06111    0.04408    0.03553         68        640: 100% 2/2 [00:00<00:00,  2.06it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.14it/s]\n","                   all         31         91       0.11      0.447      0.149     0.0455\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     33/999      7.37G    0.07124    0.03886    0.03542         76        640: 100% 2/2 [00:00<00:00,  2.55it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.07it/s]\n","                   all         31         91       0.11      0.447      0.149     0.0455\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     34/999      7.37G    0.06904    0.03657    0.03549         64        640: 100% 2/2 [00:00<00:00,  2.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.57it/s]\n","                   all         31         91      0.163      0.454      0.236     0.0886\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     35/999      7.37G    0.07155    0.04107    0.03512         95        640: 100% 2/2 [00:01<00:00,  1.95it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.18s/it]\n","                   all         31         91      0.177      0.511      0.243     0.0864\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     36/999      7.37G    0.06189    0.04138    0.03545         84        640: 100% 2/2 [00:00<00:00,  2.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.27it/s]\n","                   all         31         91      0.177      0.511      0.243     0.0864\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     37/999      7.37G    0.06333    0.03667    0.03514         67        640: 100% 2/2 [00:00<00:00,  2.32it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.29it/s]\n","                   all         31         91      0.146      0.531        0.2      0.062\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     38/999      7.37G    0.06435    0.04154    0.03515         80        640: 100% 2/2 [00:00<00:00,  2.33it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.37it/s]\n","                   all         31         91      0.118      0.386      0.146     0.0386\n","Stopping training early as no improvement observed in last 7 epochs. Best results observed at epoch 31, best model saved as best.pt.\n","To update EarlyStopping(patience=7) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n","\n","39 epochs completed in 0.038 hours.\n","Optimizer stripped from trained/train_chim_ju_kian/weights/last.pt, 42.2MB\n","Optimizer stripped from trained/train_chim_ju_kian/weights/best.pt, 42.2MB\n","\n","Validating trained/train_chim_ju_kian/weights/best.pt...\n","Fusing layers... \n","YOLOv5m summary: 212 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.42it/s]\n","                   all         31         91      0.187      0.632      0.259     0.0889\n","                  chim         31         31       0.21      0.774      0.327     0.0948\n","                    ju         31         32      0.176      0.656      0.236     0.0802\n","                  kian         31         28      0.175      0.464      0.213     0.0916\n","Results saved to \u001b[1mtrained/train_chim_ju_kian\u001b[0m\n"]}]},{"cell_type":"code","source":["## 추가 학습\n","!cd yolov5; python train.py \\\n","    --data '/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian.yaml' \\\n","    --cfg '/content/yolov5/models/yolov5m.yaml' \\\n","    --weights '/content/yolov5/trained/train_chim_ju_kian2/weights/best.pt' \\\n","    --epochs 1000 \\\n","    --patience 7 \\\n","    --project 'trained' \\\n","    --name 'train_chim_ju_kian'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNDa3dLkHqRh","executionInfo":{"status":"ok","timestamp":1679032585262,"user_tz":-540,"elapsed":74595,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"1f391eaa-affe-4b12-aba8-f36186aff0b8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mtrain: \u001b[0mweights=/content/yolov5/trained/train_chim_ju_kian2/weights/best.pt, cfg=/content/yolov5/models/yolov5m.yaml, data=/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=1000, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=trained, name=train_chim_ju_kian, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=7, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n","\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n","\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir trained', view at http://localhost:6006/\n","2023-03-17 05:55:18.425884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-03-17 05:55:19.427424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-17 05:55:19.427575: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.9/dist-packages/cv2/../../lib64:/usr/lib64-nvidia\n","2023-03-17 05:55:19.427597: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Overriding model.yaml nc=80 with nc=3\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n","  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n","  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n","  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n","  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n","  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n","  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n","  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n","  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n","  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n"," 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n"," 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n"," 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n"," 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n"," 24      [17, 20, 23]  1     32328  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n","YOLOv5m summary: 291 layers, 20879400 parameters, 20879400 gradients, 48.2 GFLOPs\n","\n","Transferred 480/481 items from /content/yolov5/trained/train_chim_ju_kian2/weights/best.pt\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/labels/train.cache... 31 images, 0 backgrounds, 0 corrupt: 100% 31/31 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/labels/train.cache... 31 images, 0 backgrounds, 0 corrupt: 100% 31/31 [00:00<?, ?it/s]\n","\n","\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.76 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n","Plotting labels to trained/train_chim_ju_kian3/labels.jpg... \n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mtrained/train_chim_ju_kian3\u001b[0m\n","Starting training for 1000 epochs...\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      0/999      6.09G    0.04655    0.04437     0.0351         77        640: 100% 2/2 [00:04<00:00,  2.08s/it]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.29s/it]\n","                   all         31         91      0.231      0.861      0.321      0.168\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      1/999      7.37G    0.04664    0.04298    0.03558         82        640: 100% 2/2 [00:01<00:00,  1.76it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.11s/it]\n","                   all         31         91      0.247       0.89      0.334      0.179\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      2/999      7.37G    0.04345    0.04728     0.0355         82        640: 100% 2/2 [00:00<00:00,  2.11it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.21it/s]\n","                   all         31         91      0.255      0.855      0.332       0.18\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      3/999      7.37G    0.04355    0.04278    0.03566         83        640: 100% 2/2 [00:00<00:00,  2.18it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.19it/s]\n","                   all         31         91      0.254      0.846      0.334      0.184\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      4/999      7.37G    0.04416    0.04062    0.03537         80        640: 100% 2/2 [00:01<00:00,  1.77it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.19s/it]\n","                   all         31         91      0.274      0.867      0.341      0.196\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      5/999      7.37G    0.04257    0.04143    0.03539         80        640: 100% 2/2 [00:00<00:00,  2.13it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.24it/s]\n","                   all         31         91       0.25      0.854      0.341      0.172\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      6/999      7.37G    0.04663    0.04751    0.03552         94        640: 100% 2/2 [00:00<00:00,  2.17it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.45it/s]\n","                   all         31         91      0.263       0.89      0.322       0.18\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      7/999      7.37G    0.04226    0.03929    0.03509         73        640: 100% 2/2 [00:00<00:00,  2.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.35it/s]\n","                   all         31         91      0.231      0.867       0.31      0.167\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      8/999      7.37G    0.04571    0.04181    0.03514         81        640: 100% 2/2 [00:00<00:00,  2.34it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.01s/it]\n","                   all         31         91      0.232      0.855      0.318      0.167\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","      9/999      7.37G    0.04865    0.04077    0.03516         79        640: 100% 2/2 [00:01<00:00,  1.99it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:01<00:00,  1.09s/it]\n","                   all         31         91      0.214      0.841      0.276      0.141\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     10/999      7.37G    0.04878    0.03834    0.03535         78        640: 100% 2/2 [00:00<00:00,  2.37it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.36it/s]\n","                   all         31         91      0.214      0.854      0.274      0.142\n","\n","      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n","     11/999      7.37G    0.04558     0.0378    0.03563         68        640: 100% 2/2 [00:00<00:00,  2.35it/s]\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.32it/s]\n","                   all         31         91       0.24      0.779      0.326      0.141\n","Stopping training early as no improvement observed in last 7 epochs. Best results observed at epoch 4, best model saved as best.pt.\n","To update EarlyStopping(patience=7) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n","\n","12 epochs completed in 0.012 hours.\n","Optimizer stripped from trained/train_chim_ju_kian3/weights/last.pt, 42.2MB\n","Optimizer stripped from trained/train_chim_ju_kian3/weights/best.pt, 42.2MB\n","\n","Validating trained/train_chim_ju_kian3/weights/best.pt...\n","Fusing layers... \n","YOLOv5m summary: 212 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 1/1 [00:00<00:00,  1.42it/s]\n","                   all         31         91      0.276      0.869      0.342      0.197\n","                  chim         31         31      0.274      0.935      0.326      0.185\n","                    ju         31         32      0.281      0.844       0.42      0.258\n","                  kian         31         28      0.272      0.827      0.281      0.148\n","Results saved to \u001b[1mtrained/train_chim_ju_kian3\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### 4) **테스트용 이미지 다운로드**"],"metadata":{"id":"_pO6Pn3JhSuP"}},{"cell_type":"code","source":[],"metadata":{"id":"g4mt9MBihSV9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5) detect.py 실행\n","\n","- 명령어 도움말 : python detect.py -h"],"metadata":{"id":"8oPYiciurwMa"}},{"cell_type":"code","source":["!cd yolov5; python detect.py \\\n","    --weights '/content/yolov5/trained/train_chim_ju_kian/weights/best.pt' \\\n","    --source '/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train' \\\n","    --project '/content/yolov5/detected' \\\n","    --name 'images' \\\n","    --line-thickness 2 \\\n","    --conf-thres 0.1 \\\n","    --iou-thres 0.5 \n","    # --img\n","    # --exist-ok \n","    # --device CPU"],"metadata":{"id":"2oQICMMmma_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679031679115,"user_tz":-540,"elapsed":9646,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"607692e8-ab9b-4d2f-f5fe-51a3b83db4e0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/trained/train_chim_ju_kian/weights/best.pt'], source=/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.1, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/yolov5/detected, name=images, exist_ok=False, line_thickness=2, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5m summary: 212 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n","image 1/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/05594179e7d6f791ef277f646917d228049eeafac2efb7b6cbccc8abc08a3688.jpg: 640x512 3 chims, 1 ju, 1 kian, 22.1ms\n","image 2/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/14654400_733109_4032_org.png: 512x640 2 chims, 8 jus, 7 kians, 23.3ms\n","image 3/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/1545822901.jpg: 640x640 6 chims, 6 jus, 3 kians, 28.7ms\n","image 4/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/1RVJB2EQS2_1.jpg: 416x640 4 chims, 7 jus, 2 kians, 21.9ms\n","image 5/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/20200121512680.jpg: 640x480 6 chims, 7 jus, 24.7ms\n","image 6/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/20200121513372.jpg: 384x640 1 chim, 1 ju, 1 kian, 23.4ms\n","image 7/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/2023012821365931312_1.jpg: 448x640 5 chims, 9 jus, 4 kians, 21.2ms\n","image 8/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309144_2843.jpg: 384x640 6 chims, 11 jus, 1 kian, 22.6ms\n","image 9/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309150_581.png: 384x640 2 chims, 3 jus, 2 kians, 18.2ms\n","image 10/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309152_5952.png: 384x640 4 chims, 6 jus, 1 kian, 20.3ms\n","image 11/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309153_5957.png: 384x640 3 chims, 18.2ms\n","image 12/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309155_438.png: 384x640 3 chims, 1 ju, 1 kian, 18.2ms\n","image 13/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/22fa5f812b9c8feda04310dff0136eb8.jpg: 384x640 2 chims, 3 jus, 18.2ms\n","image 14/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/51fa841c01460500fc0f14f2e4d28aae.jpg: 384x640 2 chims, 1 ju, 18.2ms\n","image 15/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/6e60d1eed6eab2bdf2dc0630ceb3515c.jpg: 384x640 4 chims, 4 jus, 2 kians, 18.2ms\n","image 16/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/99b983892094b5c6d2fc3736e15da7d1.jpg: 384x640 3 chims, 2 jus, 4 kians, 23.3ms\n","image 17/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/BF.32479370.1.jpg: 416x640 5 chims, 8 jus, 3 kians, 18.1ms\n","image 18/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/aa77bf71-eb59-4b2e-b03e-d8f5011f29ce.jpg: 352x640 4 chims, 5 jus, 3 kians, 20.5ms\n","image 19/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/image_readtop_2021_151143_16134369194542186.jpg: 384x640 2 chims, 2 jus, 2 kians, 16.8ms\n","image 20/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20220920200825_c93c815a.png: 352x640 3 chims, 1 ju, 3 kians, 16.8ms\n","image 21/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20220926151333_th4rhoqv.png: 480x640 7 chims, 3 jus, 4 kians, 20.0ms\n","image 22/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20221017175044_0b1053d6.png: 640x640 2 chims, 2 jus, 1 kian, 22.3ms\n","image 23/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20221017175057_be4cca39.png: 640x640 5 chims, 4 jus, 1 kian, 22.1ms\n","image 24/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/maxresdefault (1).jpg: 384x640 4 chims, 12 jus, 4 kians, 18.6ms\n","image 25/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/maxresdefault.jpg: 384x640 4 chims, 7 jus, 3 kians, 16.8ms\n","image 26/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/news_315588_1511417201_m.jpg: 512x640 3 chims, 2 jus, 2 kians, 21.8ms\n","image 27/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/news_640030_1624548215_m.jpg: 512x640 3 chims, 2 jus, 3 kians, 20.9ms\n","image 28/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/optimize.jpg: 384x640 4 chims, 3 jus, 1 kian, 18.7ms\n","image 29/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드 (1).jpg: 448x640 4 chims, 2 jus, 2 kians, 17.4ms\n","image 30/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드 (2).jpg: 640x640 1 chim, 1 ju, 2 kians, 22.3ms\n","image 31/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드.jpg: 640x640 1 chim, 3 kians, 22.1ms\n","Speed: 0.4ms pre-process, 20.5ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1m/content/yolov5/detected/images4\u001b[0m\n"]}]},{"cell_type":"code","source":["!cd yolov5; python detect.py \\\n","    --weights '/content/yolov5/trained/train_chim_ju_kian3/weights/best.pt' \\\n","    --source '/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train' \\\n","    --project '/content/yolov5/detected' \\\n","    --name 'images' \\\n","    --line-thickness 2 \\\n","    --conf-thres 0.2 \\\n","    --iou-thres 0.5 \n","    # --img\n","    # --exist-ok \n","    # --device CPU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdZVRhkPI-Bu","executionInfo":{"status":"ok","timestamp":1679032618411,"user_tz":-540,"elapsed":10465,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"1d430654-3979-47f6-ac40-eec9076e0d91"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/trained/train_chim_ju_kian3/weights/best.pt'], source=/content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.2, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/content/yolov5/detected, name=images, exist_ok=False, line_thickness=2, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 🚀 v7.0-120-g3e55763 Python-3.9.16 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5m summary: 212 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n","image 1/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/05594179e7d6f791ef277f646917d228049eeafac2efb7b6cbccc8abc08a3688.jpg: 640x512 3 chims, 3 jus, 1 kian, 26.5ms\n","image 2/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/14654400_733109_4032_org.png: 512x640 4 chims, 4 jus, 4 kians, 24.0ms\n","image 3/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/1545822901.jpg: 640x640 5 chims, 4 jus, 2 kians, 28.7ms\n","image 4/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/1RVJB2EQS2_1.jpg: 416x640 3 chims, 3 jus, 1 kian, 22.0ms\n","image 5/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/20200121512680.jpg: 640x480 4 chims, 4 jus, 22.1ms\n","image 6/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/20200121513372.jpg: 384x640 1 chim, 1 kian, 21.6ms\n","image 7/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/2023012821365931312_1.jpg: 448x640 4 chims, 4 jus, 4 kians, 21.8ms\n","image 8/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309144_2843.jpg: 384x640 5 chims, 4 jus, 18.3ms\n","image 9/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309150_581.png: 384x640 2 chims, 2 jus, 2 kians, 18.3ms\n","image 10/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309152_5952.png: 384x640 3 chims, 3 jus, 1 kian, 18.1ms\n","image 11/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309153_5957.png: 384x640 3 chims, 1 ju, 17.4ms\n","image 12/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/203707_309155_438.png: 384x640 3 chims, 3 jus, 1 kian, 17.4ms\n","image 13/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/22fa5f812b9c8feda04310dff0136eb8.jpg: 384x640 3 chims, 3 jus, 1 kian, 17.4ms\n","image 14/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/51fa841c01460500fc0f14f2e4d28aae.jpg: 384x640 3 chims, 2 jus, 1 kian, 17.3ms\n","image 15/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/6e60d1eed6eab2bdf2dc0630ceb3515c.jpg: 384x640 3 chims, 2 jus, 2 kians, 18.2ms\n","image 16/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/99b983892094b5c6d2fc3736e15da7d1.jpg: 384x640 3 chims, 2 jus, 3 kians, 17.4ms\n","image 17/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/BF.32479370.1.jpg: 416x640 3 chims, 3 jus, 3 kians, 17.8ms\n","image 18/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/aa77bf71-eb59-4b2e-b03e-d8f5011f29ce.jpg: 352x640 4 chims, 3 jus, 2 kians, 28.9ms\n","image 19/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/image_readtop_2021_151143_16134369194542186.jpg: 384x640 1 chim, 1 ju, 1 kian, 17.5ms\n","image 20/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20220920200825_c93c815a.png: 352x640 2 chims, 3 kians, 20.0ms\n","image 21/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20220926151333_th4rhoqv.png: 480x640 4 chims, 4 jus, 4 kians, 24.0ms\n","image 22/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20221017175044_0b1053d6.png: 640x640 1 chim, 1 kian, 23.1ms\n","image 23/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/img_20221017175057_be4cca39.png: 640x640 3 chims, 4 jus, 23.9ms\n","image 24/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/maxresdefault (1).jpg: 384x640 5 chims, 4 jus, 3 kians, 17.4ms\n","image 25/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/maxresdefault.jpg: 384x640 4 chims, 4 jus, 3 kians, 17.4ms\n","image 26/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/news_315588_1511417201_m.jpg: 512x640 3 chims, 3 jus, 2 kians, 18.6ms\n","image 27/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/news_640030_1624548215_m.jpg: 512x640 4 chims, 3 jus, 1 kian, 18.5ms\n","image 28/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/optimize.jpg: 384x640 3 chims, 3 jus, 1 kian, 17.4ms\n","image 29/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드 (1).jpg: 448x640 2 chims, 2 jus, 2 kians, 16.3ms\n","image 30/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드 (2).jpg: 640x640 1 chim, 1 kian, 20.4ms\n","image 31/31 /content/drive/MyDrive/my_data/object_detection_self2/chim_ju_kian/images/train/다운로드.jpg: 640x640 3 kians, 20.3ms\n","Speed: 0.5ms pre-process, 20.3ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1m/content/yolov5/detected/images5\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## Detect Image 살펴보기"],"metadata":{"id":"Js3aCClPu4mE"}},{"cell_type":"code","source":["from IPython.display import Image\n","from google.colab import files"],"metadata":{"id":"_y52SsCK9i2m","executionInfo":{"status":"ok","timestamp":1679031288554,"user_tz":-540,"elapsed":477,"user":{"displayName":"박지환","userId":"01747138341474302424"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/detected_chim_ju_kian2.zip /content/yolov5/detected/images2"],"metadata":{"id":"3mnQXY3xwfkt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/detected_chim_ju_kian3.zip /content/yolov5/detected/images3"],"metadata":{"id":"6g_UA1eDEhaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/detected_chim_ju_kian4.zip /content/yolov5/detected/images4"],"metadata":{"id":"WCI-t4SNFlaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/detected_chim_ju_kian5.zip /content/yolov5/detected/images5"],"metadata":{"id":"99W-wse7JLDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files.download(filename='/content/detected_chim_ju_kian2.zip')\n","files.download(filename='/content/detected_chim_ju_kian3.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"6fKuYA7N9d6z","executionInfo":{"status":"ok","timestamp":1679031425646,"user_tz":-540,"elapsed":287,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"9dbc95b5-608f-4c2f-9518-8b04dad0b5fc"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0408a5f9-242d-43d9-8645-090552d2190d\", \"detected_chim_ju_kian2.zip\", 7180054)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_87f525ea-98ee-48f4-bcbd-b3bc5ae391be\", \"detected_chim_ju_kian3.zip\", 7417190)"]},"metadata":{}}]},{"cell_type":"code","source":["files.download(filename='/content/detected_chim_ju_kian4.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"GMtRxBi39hEo","executionInfo":{"status":"ok","timestamp":1679031699192,"user_tz":-540,"elapsed":5,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"f98f1710-3eb5-497f-cc42-908c413a34ea"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_aba37eb6-4678-4a70-b56c-96ca322fd70a\", \"detected_chim_ju_kian4.zip\", 7425730)"]},"metadata":{}}]},{"cell_type":"code","source":["files.download(filename='/content/detected_chim_ju_kian5.zip')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"MxD9HByhFoRf","executionInfo":{"status":"ok","timestamp":1679032688469,"user_tz":-540,"elapsed":389,"user":{"displayName":"박지환","userId":"01747138341474302424"}},"outputId":"4798d137-cd6c-4849-87bd-587fc6e7f8b9"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_81c0c52a-4e73-4bf0-884e-9deb3a6c0e86\", \"detected_chim_ju_kian5.zip\", 7339699)"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"6FNdwNLaJQ0M"},"execution_count":null,"outputs":[]}]}